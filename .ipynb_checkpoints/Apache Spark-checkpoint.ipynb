{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c66c9a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488511 sha256=34ddc820719f391eb6006927ab86b339f17450873b7e857c64a6930d5264348c\n",
      "  Stored in directory: c:\\users\\omar\\appdata\\local\\pip\\cache\\wheels\\92\\09\\11\\aa01d01a7f005fda8a66ad71d2be7f8aa341bddafb27eee3c7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e38eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284c2bc",
   "metadata": {},
   "source": [
    "In this exercise, you will create the Spark Context and initialize the Spark session needed for SparkSQL and DataFrames.\n",
    "SparkContext is the entry point for Spark applications and contains functions to create RDDs such as `parallelize()`. SparkSession is needed for SparkSQL and DataFrame operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abfe73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark context class\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ef104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize spark session\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a350e",
   "metadata": {},
   "source": [
    "## Exercise 2: RDDs\n",
    "In this exercise we work with Resilient Distributed Datasets (RDDs). RDDs are Spark's primitive data abstraction and we use concepts from functional programming to create and manipulate RDDs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = range(1,30)\n",
    "# print first element of iterator\n",
    "print(data[0])\n",
    "len(data)\n",
    "xrangeRDD = sc.parallelize(data, 4)\n",
    "\n",
    "# this will let us know that we created an RDD\n",
    "xrangeRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "subRDD = xrangeRDD.map(lambda x: x-1)\n",
    "filteredRDD = subRDD.filter(lambda x : x<10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa57422",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filteredRDD.collect())\n",
    "filteredRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf530e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "test = sc.parallelize(range(1,50000),4)\n",
    "test.cache()\n",
    "\n",
    "t1 = time.time()\n",
    "# first count will trigger evaluation of count *and* cache\n",
    "count1 = test.count()\n",
    "dt1 = time.time() - t1\n",
    "print(\"dt1: \", dt1)\n",
    "\n",
    "\n",
    "t2 = time.time()\n",
    "# second count operates on cached data only\n",
    "count2 = test.count()\n",
    "dt2 = time.time() - t2\n",
    "print(\"dt2: \", dt2)\n",
    "\n",
    "#test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d411a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data first into a local `people.json` file\n",
    "!curl https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/people.json >> people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae9fb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset into a spark dataframe using the `read.json()` function\n",
    "df = spark.read.json(\"people.json\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b0f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe as well as the data schema\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d72ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5f29f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83da971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and show basic data columns\n",
    "\n",
    "df.select(\"name\").show()\n",
    "df.select(df[\"name\"]).show()\n",
    "spark.sql(\"SELECT name FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62962035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform basic filtering\n",
    "\n",
    "df.filter(df[\"age\"] > 21).show()\n",
    "spark.sql(\"SELECT age, name FROM people WHERE age > 21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e31ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfom basic aggregation of data\n",
    "\n",
    "df.groupBy(\"age\").count().show()\n",
    "spark.sql(\"SELECT age, COUNT(age) as count FROM people GROUP BY age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b892390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code\n",
    "numbers = range(1, 50)\n",
    "numbers_RDD = sc.parallelize(numbers, 4)\n",
    "even_numbers_RDD = numbers_RDD.map(lambda x: 2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bc16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code block for learners to answer\n",
    "print( even_numbers_RDD.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e83fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1152486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae1a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"people2.json\").cache()\n",
    "df.createTempView(\"people2\")\n",
    "result = spark.sql(\"SELECT AVG(age) from people2\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035bb5c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14044/2218380179.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5939c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
